{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0c727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get rid of config paths make it config object\n",
    "# TODO make the auth keys a part of config object\n",
    "# TODO give a feature of model selction\n",
    "# TODO how to add file filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apps.base.api_aggregation.loggers import Loggers\n",
    "import asyncio\n",
    "import json\n",
    "from typing import AsyncGenerator, Dict, Any\n",
    "from gpt_researcher import GPTResearcher\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from apps.langflow.models import DeepResearchConversationMessages\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "class SSELogStreamer:\n",
    "    \"\"\"\n",
    "    Custom WebSocket replacement for GPTResearcher to emit real-time steps via async queue.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.queue = asyncio.Queue()\n",
    "\n",
    "    async def send_json(self, data: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Called by GPTResearcher during research.\n",
    "        Sends intermediate agent steps, log updates, etc.\n",
    "        \"\"\"\n",
    "        # print(\"SSELogStreamer:\", data)\n",
    "        await self.queue.put(data)\n",
    "\n",
    "    async def get_next(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Await next log from queue.\n",
    "        \"\"\"\n",
    "        return await self.queue.get()\n",
    "\n",
    "def get_researcher(\n",
    "    mode: str,\n",
    "    query: str,\n",
    "    report_type: str,\n",
    "    tone: str,\n",
    "    websocket: SSELogStreamer\n",
    ") -> GPTResearcher:\n",
    "    match mode:\n",
    "        case \"web\":\n",
    "            researcher = GPTResearcher(\n",
    "                query,\n",
    "                report_type,\n",
    "                \"markdown\",\n",
    "                \"web\",\n",
    "                tone,\n",
    "                config_path='./configs/web-config.json',\n",
    "                websocket=websocket\n",
    "            )\n",
    "\n",
    "        case \"internal\":\n",
    "            client = QdrantClient(\n",
    "                url=\"https://<domain>:<port>\",\n",
    "                api_key=\"<your-api-key>\",\n",
    "            )\n",
    "            vector_store = QdrantVectorStore(\n",
    "                client=client,\n",
    "                collection_name=\"<collection_name>\",\n",
    "                embedding=BedrockEmbeddings(region_name=\"<region>\", model_id=\"<BEDROCK_MODEL_ID>\"),\n",
    "            )\n",
    "            researcher = GPTResearcher(\n",
    "                query,\n",
    "                report_type,\n",
    "                \"markdown\",\n",
    "                \"langchain_vectorstore\",\n",
    "                tone,\n",
    "                vector_store=vector_store,\n",
    "                config_path=\"./configs/local-config.json\",\n",
    "                websocket=websocket\n",
    "            )\n",
    "\n",
    "        case \"hybrid\":\n",
    "            client = QdrantClient(\n",
    "                url=\"https://<domain>:<port>\",\n",
    "                api_key=\"<your-api-key>\",\n",
    "            )\n",
    "            # TODO how to add file filters?\n",
    "            vector_store = QdrantVectorStore(\n",
    "                client=client,\n",
    "                collection_name=\"<collection_name>\",\n",
    "                embedding=BedrockEmbeddings(region_name=\"<region>\", model_id=\"<BEDROCK_MODEL_ID>\"),\n",
    "            )\n",
    "            researcher = GPTResearcher(\n",
    "                query,\n",
    "                report_type,\n",
    "                \"markdown\",\n",
    "                \"hybrid\",\n",
    "                tone,\n",
    "                vector_store=vector_store,\n",
    "                config_path=\"./configs/hybrid-config.json\",\n",
    "                websocket=websocket\n",
    "            )\n",
    "    \n",
    "    return researcher\n",
    "\n",
    "async def run_gpt_researcher_streaming(\n",
    "    conversation_id: int,\n",
    "    mode: str,\n",
    "    query: str,\n",
    "    report_type: str,\n",
    "    report_source: str,\n",
    "    tone: str,\n",
    ") -> AsyncGenerator[str, None]:\n",
    "    log_streamer = SSELogStreamer()\n",
    "    accumulated_logs = []\n",
    "\n",
    "    researcher = get_researcher(\n",
    "        mode,\n",
    "        query,\n",
    "        report_type,\n",
    "        tone,\n",
    "        log_streamer\n",
    "    )\n",
    "\n",
    "    research_task = asyncio.create_task(researcher.conduct_research())\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            while not log_streamer.queue.empty():\n",
    "                log = await log_streamer.get_next()\n",
    "\n",
    "                if log.get(\"type\") == \"logs\":\n",
    "                    log_str = log.get(\"output\", \"\")\n",
    "                    accumulated_logs.append(log_str)\n",
    "\n",
    "                yield f\"data: {json.dumps({'event': 'step', 'data': log})}\\n\\n\"\n",
    "\n",
    "            if research_task.done():\n",
    "                break\n",
    "\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "        await research_task\n",
    "\n",
    "        write_task = asyncio.create_task(researcher.write_report())\n",
    "\n",
    "        while True:\n",
    "            while not log_streamer.queue.empty():\n",
    "                log = await log_streamer.get_next()\n",
    "\n",
    "                if log.get(\"type\") == \"logs\":\n",
    "                    log_str = log.get(\"output\", \"\")\n",
    "                    accumulated_logs.append(log_str)\n",
    "\n",
    "                yield f\"data: {json.dumps({'event': 'step', 'data': log})}\\n\\n\"\n",
    "\n",
    "            if write_task.done():\n",
    "                break\n",
    "\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "        final_report = await write_task\n",
    "\n",
    "        # Flush any remaining logs\n",
    "        while not log_streamer.queue.empty():\n",
    "            log = await log_streamer.get_next()\n",
    "\n",
    "            if log.get(\"type\") == \"logs\":\n",
    "                log_str = log.get(\"output\", \"\")\n",
    "                accumulated_logs.append(log_str)\n",
    "\n",
    "            yield f\"data: {json.dumps({'event': 'step', 'data': log})}\\n\\n\"\n",
    "\n",
    "        yield f\"data: {json.dumps({'event': 'final_report', 'data': final_report})}\\n\\n\"\n",
    "\n",
    "        Loggers.log_statement(message=\"Saving the logs and final report to DB\")\n",
    "\n",
    "        # Persist to DB using sync_to_async\n",
    "        await sync_to_async(DeepResearchConversationMessages.objects.create)(\n",
    "            sender=\"AI\",\n",
    "            message={\"message\": final_report, \"logs\": \"\\n\".join(accumulated_logs)},\n",
    "            conversation_id=conversation_id\n",
    "        )\n",
    "\n",
    "    except asyncio.CancelledError:\n",
    "        yield f\"data: {json.dumps({'event': 'error', 'data': 'Streaming interrupted'})}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b0860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8948e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8ee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
